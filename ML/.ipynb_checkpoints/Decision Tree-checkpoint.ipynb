{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fa89bbc",
   "metadata": {},
   "source": [
    "# Decision Tree\n",
    "\n",
    "\n",
    "Decision trees can be used for both classification and regression problems. In classification problems, the class label is a categorical variable, and in regression problems, the target variable is a continuous variable.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. Entropy: It measures the impurity or disorder of a set of samples. A low entropy indicates high purity, and a high entropy indicates high impurity.\n",
    "\n",
    "2. Information Gain: It quantifies the reduction in entropy achieved after splitting a dataset based on an attribute. It helps in deciding the attribute to split on, with higher information gain being preferred.\n",
    "\n",
    "3. Gini Index: It measures the impurity of a set of samples, similar to entropy. Gini index values range from 0 to 1, with 0 indicating pure nodes and 1 indicating impure nodes.\n",
    "\n",
    "4. Pruning: It is a technique used to reduce the complexity of a decision tree by removing unnecessary branches or nodes. Pruning helps prevent overfitting and improves the generalization ability of the model.\n",
    "\n",
    "### Formula:\n",
    "\n",
    "-   **Entropy**: a measure of impurity in a set of examples.\n",
    "$$Entropy = -\\sum_{i=1}^n p_i log_2(p_i)$$\n",
    "\n",
    "\n",
    "-   **Information gain**: the expected reduction in entropy by splitting a set of examples based on an attribute.\n",
    "\n",
    "$$\\text{Information Gain} = H(D) - \\sum_{i=1}^{n}\\left(\\frac{|D_i|}{|D|} \\cdot H(D_i)\\right)$$\n",
    "- **Gini index**: a measure of impurity in a set of examples, similar to entropy.\n",
    "\n",
    "$$ \\text{Gini Index} = \\sum_{i=1}^n p_i (1-p_i)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752ee5d1",
   "metadata": {},
   "source": [
    "### Selection of attribute\n",
    "\n",
    "For selecting the best attribute to split in a decision tree typically involves evaluating different attributes based on certain criteria, such as information gain or Gini index. The general steps for attribute selection are as follows:\n",
    "\n",
    "1. Calculate the impurity or uncertainty measure of the current node. This could be entropy or Gini index.\n",
    "\n",
    "2. Iterate over each attribute and calculate the impurity-based criterion for splitting, such as information gain or Gini gain.\n",
    "\n",
    "3. Select the attribute with the highest information gain or lowest impurity measure. This attribute will have the most significant impact on reducing the overall impurity in the subsequent splits.\n",
    "\n",
    "4. Split the dataset based on the selected attribute and create child nodes.\n",
    "\n",
    "Recursively repeat the above steps for each child node until a stopping criterion is met, such as reaching a maximum depth, minimum number of samples, or impurity below a certain threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aa9b2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decision_tree.pdf'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import export_graphviz\n",
    "import graphviz\n",
    "\n",
    "# Load the dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a decision tree classifier using information gain criterion\n",
    "clf = DecisionTreeClassifier(criterion='entropy')\n",
    "\n",
    "# Train the classifier on the training set\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Visualize the decision tree\n",
    "dot_data = export_graphviz(clf, out_file=None, feature_names=iris.feature_names, class_names=iris.target_names, filled=True, rounded=True, special_characters=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"decision_tree\")\n",
    "\n",
    "# Display the decision tree\n",
    "graph.view()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
