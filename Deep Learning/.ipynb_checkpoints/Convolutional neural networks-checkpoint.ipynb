{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b2794d1",
   "metadata": {},
   "source": [
    "# Convolutional neural networks\n",
    "\n",
    "It is a type of neural network that are especially well-suited for image recognition and other problems where the input has a spatial structure. They use specific architectures and connection patterns to learn features at different scales and locations in the input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16abce8",
   "metadata": {},
   "source": [
    "- **Convolution operation:**\n",
    "\n",
    "In a convolutional layer, the convolution operation is used to <mark style=\"background: #FFB8EBA6;\">apply a set of filters to the input image</mark>. This operation can be represented mathematically as follows:\n",
    "\n",
    "$$\\begin{equation} S(i,j) = (I * K)(i,j) = \\sum_{m}\\sum_{n}I(m,n)K(i-m,j-n) \\end{equation}$$\n",
    "\n",
    "where $S(i,j)$ is the output of the convolution operation at position $(i,j)$, I is the input image, K is the filter/kernel, and * denotes the convolution operation.\n",
    "\n",
    "Similar to the process of sliding a small square horizontally and vertically, performing calculations to create a new square while also implementing zero padding to prevent the square from decreasing in size as shown in the image below:\n",
    "\n",
    "![[convolutional_Process.jpg]]\n",
    "\n",
    "- **Non-linearity:**\n",
    "\n",
    "Rectified Linear Unit (ReLU) activation:\n",
    "\n",
    "ReLU is a popular activation function used in CNNs. It is defined as follows:\n",
    "\n",
    "$$\\begin{equation} f(x) = \\max(0,x) \\end{equation}$$\n",
    "\n",
    "where x is the input to the activation function.\n",
    "\n",
    "Softmax activation:\n",
    "\n",
    "Softmax is commonly used as the activation function for the output layer in classification problems. It maps the output of the last hidden layer to a probability distribution over the classes. Mathematically, softmax can be defined as follows:\n",
    "\n",
    "$$\\begin{equation} P(y=j|x) = \\frac{\\exp(z_j)}{\\sum_{k=1}^{K}\\exp(z_k)} \\end{equation}$$\n",
    "\n",
    "where $P(y=j|x)$ is the probability of the j-th class given the input x, z_j is the j-th element of the output vector z of the last hidden layer, and K is the number of classes.\n",
    "\n",
    "-   **Pooling operation:**\n",
    "\n",
    "The pooling operation is used to downsample the feature maps obtained from the convolutional layer. The most common pooling operation is <mark style=\"background: #BBFABBA6;\">max pooling</mark>, which takes the maximum value in each pooling region. Mathematically, max pooling can be defined as follows:\n",
    "\n",
    "$$\\begin{equation} M(i,j) = \\max_{(m,n)\\in R_{i,j}}S(m,n) \\end{equation}$$\n",
    "\n",
    "where M(i,j) is the output of the pooling operation at position (i,j), R_{i,j} is the pooling region centered at (i,j), and S is the input feature map.\n",
    "\n",
    "-   **Fully Connected Layers:**\n",
    "\n",
    "After the feature maps are extracted through convolution, passed through activation functions, and downsampled through pooling, the output is flattened and passed through one or more fully connected layers to generate the final output. These layers are similar to the ones used in traditional neural networks and act as a classifier that uses the learned features to classify or identify the input image. The output of each fully connected layer is calculated as follows:\n",
    "\n",
    "$$z = Wx + b$$\n",
    "\n",
    "where $W$ is the weight matrix, $x$ is the input vector, $b$ is the bias vector, and $z$ is the output vector.\n",
    "\n",
    "The output of the last fully connected layer is passed through a softmax activation function to produce a probability distribution over the classes. Mathematically, softmax can be defined as follows:\n",
    "\n",
    "$$P(y=j|x) = \\frac{\\exp(z_j)}{\\sum_{k=1}^{K}\\exp(z_k)}$$\n",
    "\n",
    "-   **Backpropagation:**\n",
    "\n",
    "Once the output is generated, the network's weights and biases are updated using backpropagation to minimize the difference between predicted and actual outputs. This involves calculating the gradient of the loss function with respect to the model's parameters, and <mark style=\"background: #FFB86CA6;\">updating these parameters in the opposite direction of the gradient</mark>. This process is repeated over several iterations until the model converges. The gradients of the loss function with respect to the output of the last layer can be calculated as follows:\n",
    "\n",
    "$$\\delta_{i}^{(L)} = y_{i} - t_{i}$$\n",
    "\n",
    "where $\\delta_{i}^{(L)}$ is the error for the i-th output neuron, $y_{i}$ is the predicted output, and $t_{i}$ is the true output.\n",
    "\n",
    "Backpropagation is an <mark style=\"background: #ABF7F7A6;\">iterative algorithm that works by propagating the error back through the layers of the network</mark>. The error is calculated as the difference between the predicted and actual outputs, and is backpropagated through the layers to update the weights and biases. This process is computationally expensive and can be improved through techniques such as momentum and adaptive learning rates.\n",
    "\n",
    "The process of backpropagation enables the network to learn from its mistakes and improve its performance over time. The goal of backpropagation is to minimize the loss function, which measures the difference between the predicted and actual outputs. Once the loss function is minimized, the network is considered to have converged, and it can be used for making predictions on new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "155d1fb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Epoch: 1, Batch: 2000, Loss: 1.782\n",
      "Epoch: 1, Batch: 4000, Loss: 1.479\n",
      "Epoch: 1, Batch: 6000, Loss: 1.414\n",
      "Epoch: 1, Batch: 8000, Loss: 1.354\n",
      "Epoch: 1, Batch: 10000, Loss: 1.332\n",
      "Epoch: 1, Batch: 12000, Loss: 1.273\n",
      "Epoch: 2, Batch: 2000, Loss: 1.216\n",
      "Epoch: 2, Batch: 4000, Loss: 1.209\n",
      "Epoch: 2, Batch: 6000, Loss: 1.184\n",
      "Epoch: 2, Batch: 8000, Loss: 1.205\n",
      "Epoch: 2, Batch: 10000, Loss: 1.144\n",
      "Epoch: 2, Batch: 12000, Loss: 1.147\n",
      "Epoch: 3, Batch: 2000, Loss: 1.075\n",
      "Epoch: 3, Batch: 4000, Loss: 1.116\n",
      "Epoch: 3, Batch: 6000, Loss: 1.046\n",
      "Epoch: 3, Batch: 8000, Loss: 1.116\n",
      "Epoch: 3, Batch: 10000, Loss: 1.087\n",
      "Epoch: 3, Batch: 12000, Loss: 1.095\n",
      "Epoch: 4, Batch: 2000, Loss: 1.016\n",
      "Epoch: 4, Batch: 4000, Loss: 1.041\n",
      "Epoch: 4, Batch: 6000, Loss: 1.020\n",
      "Epoch: 4, Batch: 8000, Loss: 1.035\n",
      "Epoch: 4, Batch: 10000, Loss: 1.055\n",
      "Epoch: 4, Batch: 12000, Loss: 1.047\n",
      "Epoch: 5, Batch: 2000, Loss: 0.939\n",
      "Epoch: 5, Batch: 4000, Loss: 0.998\n",
      "Epoch: 5, Batch: 6000, Loss: 1.008\n",
      "Epoch: 5, Batch: 8000, Loss: 0.998\n",
      "Epoch: 5, Batch: 10000, Loss: 1.026\n",
      "Epoch: 5, Batch: 12000, Loss: 1.019\n",
      "Training finished.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Set the random seed for reproducibility. This ensures that the random numbers generated by PyTorch will be the same every time the code is run, allowing for consistent resultsduring training.\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Define the CNN architecture\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Defines the first convolutional layer. It takes input with 3 channels, applies 16 filters, uses a kernel size of 3x3, a stride of 1, and padding of 1 to maintain the spatial dimensions of the input.\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.fc = nn.Linear(16 * 16 * 16, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create an instance of the CNN\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Load the data\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "# Train the model\n",
    "for epoch in range(5):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader, 0):\n",
    "        inputs, labels = data\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:\n",
    "            print(f'Epoch: {epoch+1}, Batch: {i+1}, Loss: {running_loss/2000:.3f}')\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Training finished.')\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'cnn_model.pth')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
