{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "573fef9c",
   "metadata": {},
   "source": [
    "## RNN language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a6e881",
   "metadata": {},
   "source": [
    "An autoregressive RNN model which can generate peopleâ€™s names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26022a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import string\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f68830",
   "metadata": {},
   "source": [
    "### Create an vocabulary\n",
    "- vocabulary: special tokens + letters + numbers \n",
    "    - \"<bos>\": Beginning of sequence token,\n",
    "    - \".\" : End of sequence token, \n",
    "    - \"\": Empty string used to denote elements for the RNN to ignore\n",
    "- id_to_char: map the charater in the vocabulary by index\n",
    "- char_to_id: the opposite of id_to_char, which maps each character/token in the vocabulary to a unique integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eef8272d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vocab():\n",
    "    vocab = [\"\", \"<bos>\", \".\"] + list(string.ascii_lowercase + string.ascii_uppercase + string.digits + \" \")\n",
    "    id_to_char = {i: v for i, v in enumerate(vocab)}\n",
    "    char_to_id = {v: i for i, v in enumerate(vocab)}\n",
    "    return vocab, id_to_char, char_to_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe17725e",
   "metadata": {},
   "source": [
    "### Load data\n",
    "- sequence modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22436e4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filename):\n",
    "    data = json.load(open(filename, \"r\"))\n",
    "    data = [v+'.' for v in data]\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972383b2",
   "metadata": {},
   "source": [
    "### Convert Sequence to ID\n",
    "- convert a list of sequences to a 2D numpy array of token IDs by char_to_id\n",
    "- set a max length for ID length, truncate or padding with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c400217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def seqs_to_ids(seqs, char_to_id, max_len=20):\n",
    "    all_seqs = []\n",
    "    for name in seqs:\n",
    "        if not name:  # skip None\n",
    "            continue\n",
    "            \n",
    "        # name -> ids\n",
    "        seq_ids = [char_to_id[char] for char in name if char in char_to_id]\n",
    "\n",
    "        # Truncate or pad\n",
    "        seq_ids = seq_ids[:max_len]\n",
    "        while len(seq_ids) < max_len:\n",
    "            seq_ids.append(0)  \n",
    "        all_seqs.append(seq_ids)\n",
    "\n",
    "    return np.array(all_seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9b48b5",
   "metadata": {},
   "source": [
    "### RNN Class\n",
    "- Convert words to vectors\n",
    "- GRU\n",
    "- Linear out layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d505c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNLM(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_size = 32, gru_size=32):\n",
    "        super(RNNLM, self).__init__()\n",
    "\n",
    "        # store layer sizes\n",
    "        self.emb_size = emb_size\n",
    "        self.gru_size = gru_size\n",
    "\n",
    "        # for embedding characters (ignores those with value 0: the padded values)\n",
    "        self.emb = nn.Embedding(vocab_size, emb_size, padding_idx=0)\n",
    "        # GRU layer\n",
    "        self.gru = nn.GRU(emb_size, gru_size, batch_first=True)\n",
    "        # linear layer for output\n",
    "        self.linear = nn.Linear(gru_size, vocab_size)\n",
    "    \n",
    "    def forward(self, x, h_last=None):\n",
    "        \n",
    "        # Embed, discrete -> continuous\n",
    "        embedded_seq = self.emb(x)\n",
    "\n",
    "        # Pass the embedded sequence through the GRU\n",
    "        if h_last is not None:\n",
    "            gru_out, h = self.gru(embedded_seq, h_last)\n",
    "        else:\n",
    "            gru_out, h = self.gru(embedded_seq)\n",
    "\n",
    "        # Pass the GRU's output through the linear layer to get logits\n",
    "        out = self.linear(gru_out)\n",
    "\n",
    "        return out, h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624bed21",
   "metadata": {},
   "source": [
    "### Model Trianing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2a9cba37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, Xtrain, Ytrain, Xval, Yval, id_to_char, max_epoch):\n",
    "    \n",
    "    optim = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
    "    lossfn = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "    batch_size = 32\n",
    "    num_batches = int(Xtrain.shape[0] / batch_size)\n",
    "\n",
    "    # run the main training loop over many epochs\n",
    "    for e in range(max_epoch):\n",
    "        \n",
    "        # shuffle the training data\n",
    "        permutation = torch.randperm(Xtrain.shape[0])\n",
    "        Xtrain_shuffled = Xtrain[permutation]\n",
    "        Ytrain_shuffled = Ytrain[permutation]\n",
    "\n",
    "        # iterate over the dataset in batches\n",
    "        for i in range(0, Xtrain.shape[0], batch_size):\n",
    "            # get the current batch\n",
    "            Xbatch = Xtrain_shuffled[i:i+batch_size]\n",
    "            Ybatch = Ytrain_shuffled[i:i+batch_size]\n",
    "\n",
    "            # trian\n",
    "            optim.zero_grad()\n",
    "            logits, _ = model(Xbatch)\n",
    "            loss = lossfn(logits.view(-1, logits.shape[-1]), Ybatch.view(-1))\n",
    "            loss.backward()\n",
    "            optim.step()\n",
    "\n",
    "        # after each epoch, compute the validation loss\n",
    "        with torch.no_grad():\n",
    "            logits_val, _ = model(Xval)\n",
    "            val_loss = lossfn(logits_val.view(-1, logits_val.shape[-1]), Yval.view(-1))\n",
    "            print(f\"Epoch {e+1}/{max_epoch} - Validation Loss: {val_loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a51a6ef",
   "metadata": {},
   "source": [
    "### Generate sequence of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db7feedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_string(model, id_to_char, max_len=20, sample=True):\n",
    "\n",
    "    # put the model into eval mode because we don't need gradients\n",
    "    model.eval()\n",
    "\n",
    "    # we will use a batch size of one for generation\n",
    "    x = torch.ones((1, 1), dtype=torch.long) # x is the <bos> token id which = 1\n",
    "    h = torch.zeros((1,1,model.gru_size), dtype=torch.float) # h0 is all zeros\n",
    "    out_str = \"\"\n",
    "    \n",
    "    # generate the sequence step by step\n",
    "    for i in range(max_len):\n",
    "\n",
    "        logits, h = model(x, h)\n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1)\n",
    "\n",
    "        if sample:\n",
    "            # sample from the distribution\n",
    "            next_token_id = torch.multinomial(probs[0], 1)\n",
    "        else:\n",
    "            # use argmax decoding\n",
    "            next_token_id = torch.argmax(probs[0], dim=-1)\n",
    "        \n",
    "        next_char = id_to_char[next_token_id.item()]\n",
    "        \n",
    "        # check if the end token is reached\n",
    "        if next_char == '.':  # end\n",
    "            break\n",
    "        \n",
    "        # convert the token id to a character\n",
    "        out_str += next_char\n",
    "\n",
    "        # set the input for the next iteration\n",
    "        x = next_token_id.view(1, 1)\n",
    "\n",
    "    # set the model back to training mode\n",
    "    model.train()\n",
    "\n",
    "    return out_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7167e776",
   "metadata": {},
   "source": [
    "### Val loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dc9c13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_val_loss(model, Xval, Yval):\n",
    "\n",
    "    # use cross entropy loss\n",
    "    lossfn = nn.CrossEntropyLoss(ignore_index=0, reduction='sum')\n",
    "\n",
    "    # put the model into eval mode because we don't need gradients\n",
    "    model.eval()\n",
    "\n",
    "    # calculate number of batches, we need to be precise this time\n",
    "    batch_size = 32\n",
    "    num_batches = int(Xval.shape[0] / batch_size)\n",
    "    if Xval.shape[0] % batch_size != 0:\n",
    "        num_batches += 1\n",
    "\n",
    "    # sum up the total loss\n",
    "    total_loss = 0\n",
    "    total_chars = 0\n",
    "    for n in range(num_batches):\n",
    "\n",
    "        # calculate batch start end idxs \n",
    "        s = n * batch_size\n",
    "        e = (n+1)*batch_size\n",
    "        if e > Xval.shape[0]:\n",
    "            e = Xval.shape[0]\n",
    "\n",
    "        # compute output of model        \n",
    "        out,_ = model(Xval[s:e])\n",
    "\n",
    "        # compute loss and store\n",
    "        loss = lossfn(out.permute(0, 2, 1), Yval[s:e]).detach().cpu().numpy()\n",
    "        total_loss += loss\n",
    "\n",
    "        char_count = torch.count_nonzero(Yval[s:e].flatten())\n",
    "        total_chars += char_count.detach().cpu().numpy()\n",
    "\n",
    "    # compute average loss per character\n",
    "    total_loss /= total_chars\n",
    "    \n",
    "    # set the model back to training mode in case we need gradients later\n",
    "    model.train()\n",
    "\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d21f79",
   "metadata": {},
   "source": [
    "### Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9435472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # load the data from disk\n",
    "    data = load_data(os.path.join(\"data\", \"names.json\"))\n",
    "\n",
    "    # get the letter 'vocabulary'\n",
    "    vocab, id_to_char, char_to_id = get_vocab()\n",
    "    vocab_size = len(vocab)\n",
    "\n",
    "    # convert the data into a sequence of ids\n",
    "    Y = seqs_to_ids(data, char_to_id)\n",
    "    # the input needs to be shifted by 1 and have the <bos> tokenid prepended to it\n",
    "    # this also means we have to remove the last element of the sequence to keep the length constant\n",
    "    X = np.concatenate([np.ones((Y.shape[0], 1)), Y[:, :-1]], axis=1)\n",
    "\n",
    "    # split the data int training and validation\n",
    "    # convert the data into torch tensors\n",
    "    train_frac = 0.9\n",
    "    num_train = int(X.shape[0]*train_frac)\n",
    "    Xtrain = torch.tensor(X[:num_train], dtype=torch.long)\n",
    "    Ytrain = torch.tensor(Y[:num_train], dtype=torch.long)\n",
    "    Xval = torch.tensor(X[num_train:], dtype=torch.long)\n",
    "    Yval = torch.tensor(Y[num_train:], dtype=torch.long)\n",
    "\n",
    "    # train the model\n",
    "    model = RNNLM(vocab_size)\n",
    "    train_model(model, Xtrain, Ytrain, Xval, Yval, id_to_char, max_epoch=10)\n",
    "\n",
    "    # use the model to generate and print some names\n",
    "    print(\"Argmax: \", gen_string(model, id_to_char, sample=False))\n",
    "    print(\"Random:\")\n",
    "    for i in range(10):\n",
    "        gstr = gen_string(model, id_to_char)\n",
    "        print(gstr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "128b527d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Validation Loss: 2.7959718704223633\n",
      "Epoch 2/10 - Validation Loss: 2.560406446456909\n",
      "Epoch 3/10 - Validation Loss: 2.456402540206909\n",
      "Epoch 4/10 - Validation Loss: 2.4059159755706787\n",
      "Epoch 5/10 - Validation Loss: 2.3775088787078857\n",
      "Epoch 6/10 - Validation Loss: 2.3552451133728027\n",
      "Epoch 7/10 - Validation Loss: 2.3368332386016846\n",
      "Epoch 8/10 - Validation Loss: 2.32159686088562\n",
      "Epoch 9/10 - Validation Loss: 2.3085272312164307\n",
      "Epoch 10/10 - Validation Loss: 2.2972211837768555\n",
      "Argmax:  John Marton\n",
      "Random:\n",
      "Mithake Lehid\n",
      "Al Erjerd Benettton\n",
      "Chrentrin Gdeter\n",
      "Neizari Repergum\n",
      "Div Yocott\n",
      "Carasher Farbenininb\n",
      "Ravid Vavaten\n",
      "Tmatsaid Bantharlin\n",
      "Liomailear Try\n",
      "Tonrii Walaed\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
