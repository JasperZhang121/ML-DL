{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a80d69f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import issparse, csr_matrix\n",
    "from numpy.linalg import norm\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "import nltk\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc959b1",
   "metadata": {},
   "source": [
    "## Features Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078165cc",
   "metadata": {},
   "source": [
    "#### Text processing preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0f61615",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokeniser = nltk.tokenize.TreebankWordTokenizer()\n",
    "stopwords = frozenset(nltk.corpus.stopwords.words(\"english\"))\n",
    "trans_table = str.maketrans(dict.fromkeys(string.punctuation))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1295f42a",
   "metadata": {},
   "source": [
    "#### Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7dc295d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenise_text(str_):\n",
    "    # remove non-ASCII characters for simplicity\n",
    "    str_ = str_.encode(encoding='ascii', errors='ignore').decode()\n",
    "    return [t for t in tokeniser.tokenize(str_.lower().translate(trans_table)) if t not in stopwords]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffe2857b",
   "metadata": {},
   "source": [
    "#### Extract features by TF-IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2faa8e27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_tfidf(Xr_fit, Xr_pred=None):\n",
    "    print('Generating features (TF-IDF) ...')\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_fit = vectorizer.fit_transform(Xr_fit)\n",
    "    if Xr_pred is not None:\n",
    "        X_pred = vectorizer.transform(Xr_pred)\n",
    "    return X_fit if Xr_pred is None else (X_fit, X_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60149dd4",
   "metadata": {},
   "source": [
    "#### Document -> Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92906e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_to_vector(tokenised_doc, word_vectors):\n",
    "    \"\"\"Takes a (tokenised) document and turns it into a vector by aggregating\n",
    "    its word vectors.\n",
    "\n",
    "    Args:\n",
    "        tokenised_doc (list(list(str))): A document represented as list of\n",
    "            sentences. Each sentence is a list of tokens.\n",
    "        word_vectors (gensim.models.keyedvectors.KeyedVectors): A mapping \n",
    "            from words (string) to their embeddings (np.ndarray)\n",
    "\n",
    "    Returns:\n",
    "        np.array: The aggregated word vector representing the input document.\n",
    "    \"\"\"\n",
    "    # check the input\n",
    "    assert isinstance(word_vectors, KeyedVectors)\n",
    "    vector_size = word_vectors.vector_size\n",
    "\n",
    "    vec= np.zeros(vector_size)\n",
    "    count = 0   # count words that have embeddings\n",
    "\n",
    "    for sentence in tokenised_doc:\n",
    "        for token in sentence:\n",
    "            if token in word_vectors:\n",
    "                vec += word_vectors[token]\n",
    "                count += 1\n",
    "    \n",
    "    # weighted average of word vectors\n",
    "    if count > 0:\n",
    "        vec /= count\n",
    "\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5933d3a5",
   "metadata": {},
   "source": [
    "#### Extract features by word vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4dbceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_w2v(Xt, word_vectors):\n",
    "    print('Generating features (word2vec) ...')\n",
    "    return np.vstack([document_to_vector(xt, word_vectors) for xt in tqdm(Xt)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddc027a",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b41056f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_distance(v1, v2):\n",
    "    \"\"\"Compute the cosine distance between the two input vectors.\n",
    "\n",
    "    Args:\n",
    "        v1: A (sparse or dense) vector.\n",
    "        v2: Another (sparse or dense) vector.\n",
    "\n",
    "    Returns:\n",
    "        float: The cosine distance between `v1` and `v2`.\n",
    "    \"\"\"\n",
    "\n",
    "    # If one of the vectors is sparse and the other is dense, \n",
    "    # convert the dense one to sparse.\n",
    "    if issparse(v1) and not issparse(v2):\n",
    "        v2 = csr_matrix(v2)\n",
    "    elif not issparse(v1) and issparse(v2):\n",
    "        v1 = csr_matrix(v1)\n",
    "\n",
    "    # Using scikit-learn's cosine_distances\n",
    "    distance = cosine_distances(v1.reshape(1, -1), v2.reshape(1, -1))[0, 0]\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76d1a84e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_distances(data, centroids):\n",
    "    \"\"\"compute the cosine distances between every data point and\n",
    "    every centroid.\n",
    "\n",
    "    Args:\n",
    "        data: A (sparse or dense) matrix of features for N documents.\n",
    "            Each row represents a document.\n",
    "        centroids (np.ndarray): The K cluster centres. Each row\n",
    "            represent a cluster centre.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: An N x K matrix of cosine distances.\n",
    "    \"\"\"\n",
    "    \n",
    "    assert data.shape[1] == centroids.shape[1]\n",
    "\n",
    "    N = data.shape[0]\n",
    "    K = centroids.shape[0]\n",
    "    dists = np.full((N, K), -1.)\n",
    "\n",
    "    for i in range(N):\n",
    "        for j in range(K):\n",
    "            v1 = data[i] # Ensure it's a row vector\n",
    "            v2 = centroids[j]  # Ensure it's a row vector\n",
    "            dists[i, j] = cosine_distance(v1, v2)\n",
    "    return dists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "432a1a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_data_points(distances):\n",
    "    \"\"\"Assign each data point to its closest centroid.\n",
    "\n",
    "    Args:\n",
    "        distances (np.ndarray): An N x K matrix where distances[i, j]\n",
    "            is the cosine distance between the i-th data point and\n",
    "            the j-th centroid.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: A vector of size N.\n",
    "    \"\"\"\n",
    "    N, K = distances.shape\n",
    "    clusters = np.full(N, -1)\n",
    "\n",
    "    for i in range(N):\n",
    "        clusters[i] = np.argmin(distances[i, :])\n",
    "\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8987f092",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_centroids(data, centroids, clusters):\n",
    "    \"\"\"Re-compute each centroid as the average of the data points\n",
    "    assigned to it.\n",
    "\n",
    "    Args:\n",
    "        data: A (sparse or dense) matrix of features for N documents.\n",
    "            Each row represents a document.\n",
    "        centroids (np.ndarray): The K cluster centres. Each row\n",
    "            represent a cluster centre.\n",
    "        clusters (np.ndarray): A vector of size N where clusters[i] = j\n",
    "            denotes that the i-th data point is assigned to the j-th\n",
    "            centroid.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: The updated centroids.\n",
    "    \"\"\"\n",
    "    # check the input\n",
    "    assert data.shape[1] == centroids.shape[1]\n",
    "    N = data.shape[0]\n",
    "    K = centroids.shape[0]\n",
    "    assert clusters.shape[0] == N\n",
    "\n",
    "    # Re-compute each centroid as the average of the data points assigned to it.\n",
    "    for k in range(K):\n",
    "        # Get the data points assigned to centroid k\n",
    "        assigned_points = data[clusters == k]\n",
    "        \n",
    "        # If there are no points assigned to the centroid,the centroid remains unchanged\n",
    "        if assigned_points.shape[0] > 0:\n",
    "            # Compute the mean of assigned points\n",
    "            new_centroid = np.mean(assigned_points, axis=0)\n",
    "            \n",
    "            # Update the centroid's position\n",
    "            centroids[k] = new_centroid\n",
    "\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b76b8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans(data, K, max_iter=10, rng=None):\n",
    "    \"\"\"Clustering data points using the KMeans algorithm.\n",
    "\n",
    "    Args:\n",
    "        data: A matrix of features of documents. Each row represents a document.\n",
    "        K (int): The number of cluster centres.\n",
    "        max_iter (int): The maximum number of iterations to run in the KMeans algorithm.\n",
    "        rng (np.random.Generator): A random number generator.\n",
    "\n",
    "    Returns:\n",
    "        centroids (np.ndarray): The cluster centres (after the re-computation of centroids).\n",
    "        clusters (np.ndarray): The index of cluster each document belongs to, e.g., clusters[i] = k\n",
    "            denotes that the i-th document is in the k-th cluster.\n",
    "    \"\"\"\n",
    "    print(f'Clustering using KMeans (K={K}) ...')\n",
    "    N = data.shape[0]\n",
    "    assert N >= K\n",
    "    rng = np.random.default_rng(rng)\n",
    "    indices = rng.choice(N, size=K, replace=False)\n",
    "    if issparse(data):\n",
    "        centroids = data[indices, :].A  # dense\n",
    "    else:\n",
    "        centroids = data[indices, :]\n",
    "    \n",
    "    print(f'{\"Iteration\":>10} {\"Total Distance\":>20}')\n",
    "    prev_clusters = None\n",
    "    for i in range(max_iter):\n",
    "        dists = compute_distances(data, centroids)\n",
    "        clusters = assign_data_points(dists)\n",
    "        centroids = update_centroids(data, centroids, clusters)\n",
    "        print(f'{i:>10} {round(dists.min(axis=1).sum(), 2):>20}')\n",
    "        if prev_clusters is not None and np.all(prev_clusters == clusters):\n",
    "            return centroids, clusters\n",
    "        prev_clusters = clusters\n",
    "    return centroids, clusters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
